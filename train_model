import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import pickle
import json
import random
from sklearn.utils import shuffle

#超参数
lr = 0.001
batch_size = 96
iters = 5000
view_1 = 5
view_2 = 5
view_3 = 5
view_4 = 3
view_5 = 3
num_filter_1 = 32
num_filter_2 = 64
num_filter_3 = 64
#num_filter_4 = 64
#num_filter_5 = 64
fc_neuron_num_1 = 512
#fc_neuron_num_out = 512
#------
#dropout
use_dropout = False
use_bn = False
dropout = 0.6#每个元素被保留的概率
keep_prob = tf.placeholder(tf.float32)


n = 0
t = 0
def get_train_batch():
    global n
    global t
    m = n%5 +1
    s = t%10000 
    file_name = 'data_batch_' + str(m)
    with open(file_name,'rb') as f:
        batch = pickle.load(f,encoding='bytes')
        
    t += 100
    if s == 9900:
        n += 1
    try:
        x_batch = batch[b'data'][s:s+batch_size]
        y_batch = batch[b'labels'][s:s+batch_size]
    except:
        x_batch = batch[b'data'][s:-1]
        y_batch = batch[b'labels'][s:-1]
    return x_batch,y_batch

def get_test_batch():
    with open('test_batch','rb') as f:
        batch = pickle.load(f,encoding='bytes')

    return batch[b'data'],batch[b'labels']

def roll_pics(x_batch,y_batch):
    n =0
    img_batch = x_batch
    y__batch = y_batch
    for index,img in enumerate(x_batch):
        if n%15 == 0:
            img_batch = np.append(img_batch,x_batch[index][:,::-1])
            y__batch = np.append(y__batch,y_batch[index])
        n+=1
    img_batch = img_batch.reshape(-1,32,32,3)
    y__batch = y__batch.reshape(-1,10)
    #运用sklearn.utils的shuffle能够同时打乱两个数组，并且保持label对应关系
    x,y = shuffle(img_batch,y__batch)
    return x,y

def data_batch_pretreatment(img_batch):
    img_batch = img_batch.reshape(-1,3,32,32)
    target_batch = img_batch.reshape(-1,32,32,3)
    for index,img in enumerate(img_batch):

        _r = img[0]
        _g = img[1]
        _b = img[2]

        img_r = Image.fromarray(_r)  
        img_g = Image.fromarray(_g)
        img_b = Image.fromarray(_b)

        img = Image.merge('RGB',(img_r,img_g,img_b))
        img = np.array(img)
        target_batch[index] = img     

    return target_batch

def conv2d(x,w,b,strides=1):
    x = tf.nn.conv2d(x,w,strides=[1,strides,strides,1],padding='SAME')
    x = tf.nn.bias_add(x,b)
    if use_bn:
        x = tf.layers.batch_normalization(x)
    return tf.nn.relu(x)

def maxpool2d(x,k=2):
    x = tf.nn.max_pool(x,ksize=[1,3,3,1],strides=[1,k,k,1],padding='SAME')
    return x

def conv_net(x,weights,biases):
        
    #x = tf.reshape(x,[-1,32,32,3])
    #x = x/255

    conv_1 = conv2d(x,weights['wc1'],biases['bc1'])
    conv_1 = maxpool2d(conv_1)
    #lrn1 = tf.nn.lrn(conv_1,4,bias=1,alpha=0.001/9.0, beta=0.75)
    
    conv_2 = conv2d(conv_1,weights['wc2'],biases['bc2'])
    conv_3 = conv2d(conv_2,weights['wc3'],biases['bc3'])
    conv_3 = maxpool2d(conv_3)
    #lrn2 = tf.nn.lrn(conv_2,4,bias=1,alpha=0.001/9.0, beta=0.75)

    #conv_3 = conv2d(conv_2,weights['wc3'],biases['bc3'])
    '''
    conv_4 = conv2d(conv_3,weights['wc4'],biases['bc4'])
    conv_4 = maxpool2d(conv_4)

    conv_5 = conv2d(conv_4,weights['wc5'],biases['bc5'])
    conv_5 = maxpool2d(conv_5)
    '''
    fc = tf.reshape(conv_3,[-1,8*8*num_filter_3])

    fc1 = tf.nn.relu(tf.matmul(fc,weights['wf1'])+biases['bf1'])
    if use_dropout:
        fc1 = tf.nn.dropout(fc1,keep_prob)
    '''
    fc2 = tf.nn.relu(tf.matmul(fc1,weights['wf2']) + biases['bf2'])
    if use_dropout:
        fc2 = tf.nn.dropout(fc2,keep_prob)
    '''
    out = tf.matmul(fc1,weights['out']) + biases['out']

    return out

weights={'wc1':tf.Variable(tf.random.truncated_normal([view_1,view_1,3,num_filter_1],stddev=0.05)),
         'wc2':tf.Variable(tf.random.truncated_normal([view_2,view_2,num_filter_1,num_filter_2],stddev=0.05)/np.sqrt(num_filter_1/2)),
         'wc3':tf.Variable(tf.random.truncated_normal([view_3,view_3,num_filter_2,num_filter_3],stddev=0.05)/np.sqrt(num_filter_2/2)),
         #'wc4':tf.Variable(tf.random.truncated_normal([view_4,view_4,num_filter_3,num_filter_4],stddev=0.05)/np.sqrt(num_filter_3/2)),
         #'wc5':tf.Variable(tf.random.truncated_normal([view_5,view_5,num_filter_4,num_filter_5],stddev=0.05)/np.sqrt(num_filter_4/2)),
         'wf1':tf.Variable(tf.random.truncated_normal([8*8*num_filter_3,fc_neuron_num_1],stddev=0.04)/np.sqrt(num_filter_3/2)),
         #'wf2':tf.Variable(tf.random.truncated_normal([fc_neuron_num_1,fc_neuron_num_out],stddev=0.04)/np.sqrt(fc_neuron_num_1/2)),
         'out':tf.Variable(tf.random.truncated_normal([fc_neuron_num_1,10],stddev=1/192)/np.sqrt(192/2))
         }

biases={'bc1':tf.zeros([num_filter_1]),
        'bc2':tf.zeros([num_filter_2])+0.1,
        'bc3':tf.zeros([num_filter_3]),
        #'bc4':tf.zeros([num_filter_4]),
        #'bc5':tf.zeros([num_filter_5]),
        'bf1':tf.zeros([fc_neuron_num_1])+0.1,
        #'bf2':tf.zeros([fc_neuron_num_out])+0.1,
        'out':tf.zeros([10]),
        } 
x = tf.placeholder(tf.float32,[None,32,32,3])
y = tf.placeholder(tf.float32,[None,10])

pred = conv_net(x,weights,biases)
cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
optimizer = tf.train.AdamOptimizer(lr).minimize(cost)
accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,axis=1),tf.argmax(y,axis=1)),dtype=tf.float32))
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    #Train
    for step in range(iters):
        x_batch,y_batch = get_train_batch()
        x_batch = data_batch_pretreatment(x_batch)
        y_batch = sess.run(tf.one_hot(y_batch,depth=10))
        #x_batch,y_batch = roll_pics(x_batch,y_batch)
        sess.run(optimizer,feed_dict={x:x_batch,y:y_batch,keep_prob:dropout})
        loss = sess.run(cost,feed_dict={x:x_batch,y:y_batch,keep_prob:dropout})
        acc = sess.run(accuracy,feed_dict={x:x_batch,y:y_batch,keep_prob:dropout})
        print('loss:',loss)
        print('accuracy:',acc,'\n')
    #Evaluate
    use_dropout = False
    use_bn = False
    avg_acc_test = 0
    idx = 0
    test_x,test_y = get_test_batch()
    test_x = data_batch_pretreatment(test_x)
    test_y = sess.run(tf.one_hot(test_y,depth=10))
    for i in range(100):
        acc_test = sess.run(accuracy,feed_dict={x:test_x[idx:idx+100],y:test_y[idx:idx+100],keep_prob:dropout})
        idx += 100
        avg_acc_test += acc_test
        
    avg_acc_test = avg_acc_test/100
    print('Done! Average accuracy of test data is: ',avg_acc_test)

    #保存模型变量，注意json不接受numpy的array,要变成list
    '''
    with open ('weights.json','w') as f:
        ws = {}
        for name,w in weights.items():
            ws[name] = sess.run(w).tolist()
        json.dump(ws,f)

    with open ('biases.json','w') as f:
        bs = {}
        for name,b in biases.items():
            bs[name] = sess.run(b).tolist()
        json.dump(bs,f)
            
    '''
